{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zdr-gdcKj_-"
   },
   "source": [
    "## Task 2 - Supervised Baseline [colab version]\n",
    "---\n",
    "### Author - Shourav Deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41072,
     "status": "ok",
     "timestamp": 1761924283860,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "_Ef1RD_rKKDZ",
    "outputId": "fa52d12a-8df7-476b-9996-06ebec23937f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Device: cuda\n",
      "Torch version: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import (\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    resnet50, ResNet50_Weights,\n",
    "    vit_b_16, ViT_B_16_Weights\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Mount Google Drive so we can save checkpoints permanently\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqDPe4fyK6_0"
   },
   "source": [
    "# 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1761924294462,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "_KxQUkGxKyqM",
    "outputId": "c0476d96-5e98-4b08-898f-c7e388d157cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run Train:90% / Test:10% with efficientnet_b0\n",
      "Freeze backbone: True\n",
      "Saving outputs to: /content/drive/MyDrive/Colab Notebooks/Outputs/betel_leaf_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_ROOT = \"/content/drive/MyDrive/Betel Leaf Dataset\"\n",
    "\n",
    "NUM_CLASSES   = 3\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "IMG_SIZE      = 224\n",
    "\n",
    "# Other BACKBONE\n",
    "BACKBONE = \"efficientnet_b0\"\n",
    "# BACKBONE = \"resnet50\"\n",
    "# BACKBONE = \"vit_b_16\"\n",
    "\n",
    "# We will run one split at a time.\n",
    "TRAIN_RATIO = 0.9\n",
    "TEST_RATIO  = 0.1\n",
    "assert math.isclose(TRAIN_RATIO + TEST_RATIO, 1.0, rel_tol=1e-6), \"Train+Test must = 1.0\"\n",
    "\n",
    "# Toggle for freezing backbone\n",
    "# True  -> freeze feature extractor, train only classifier (faster)\n",
    "# False -> full fine-tuning\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "# Output dir\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Outputs/betel_leaf_checkpoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Will run Train:{int(TRAIN_RATIO*100)}% / Test:{int(TEST_RATIO*100)}% with {BACKBONE}\")\n",
    "print(\"Freeze backbone:\", FREEZE_BACKBONE)\n",
    "print(\"Saving outputs to:\", OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB6M53bBLry2"
   },
   "source": [
    "# 3) Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5875,
     "status": "ok",
     "timestamp": 1761924304438,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "nGgEQEuVLly9",
    "outputId": "16b3f49d-8485-48a4-92a7-0c2818c660bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 2082\n",
      "Class counts: {'healthy': 669, 'diseased': 509, 'dried': 904}\n"
     ]
    }
   ],
   "source": [
    "# Here we normalize class names from folder names\n",
    "CLASS_MAP = {\n",
    "    \"healthy\":   [\"Healthy\", \"Healthy Leaf\"],\n",
    "    \"diseased\":  [\"Diseased\", \"Diseased Leaf\"],\n",
    "    \"dried\":     [\"Dried\", \"Dried Leaf\", \"Dried Leaf\"],\n",
    "}\n",
    "\n",
    "def collect_image_paths(dataset_root):\n",
    "    \"\"\"\n",
    "    Walk through dataset_root and return list of (img_path, class_idx, class_name_norm)\n",
    "    where class_name_norm is one of: healthy / diseased / dried\n",
    "    \"\"\"\n",
    "    all_samples = []\n",
    "    class_to_idx = {\"healthy\": 0, \"diseased\": 1, \"dried\": 2}\n",
    "\n",
    "    for class_name_norm, aliases in CLASS_MAP.items():\n",
    "        for alias in aliases:\n",
    "            pattern = os.path.join(dataset_root, \"**\", alias, \"*.*\")\n",
    "            for img_path in glob.glob(pattern, recursive=True):\n",
    "                if img_path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")):\n",
    "                    all_samples.append(\n",
    "                        (img_path, class_to_idx[class_name_norm], class_name_norm)\n",
    "                    )\n",
    "    return all_samples, class_to_idx\n",
    "\n",
    "all_samples, class_to_idx = collect_image_paths(DATASET_ROOT)\n",
    "\n",
    "print(f\"Total images found: {len(all_samples)}\")\n",
    "class_counts = defaultdict(int)\n",
    "for _, idx, cname in all_samples:\n",
    "    class_counts[cname] += 1\n",
    "print(\"Class counts:\", dict(class_counts))\n",
    "\n",
    "\n",
    "class BetelLeafDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        \"\"\"\n",
    "        samples: list of (img_path, class_idx, class_name_norm)\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_path, class_idx, class_name = self.samples[i]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P0ccQXfL0rF"
   },
   "source": [
    "# 4) Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1761924306973,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "kg9TZMVkL0Jt"
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.2),\n",
    "    T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fnTNQjsL_pc"
   },
   "source": [
    "# 5) Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1761924310604,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "ujIW-tiwMAG9"
   },
   "outputs": [],
   "source": [
    "def get_model(backbone_name, num_classes=3):\n",
    "    \"\"\"\n",
    "    Returns a model with pretrained weights and replaces\n",
    "    the classifier head with (num_classes).\n",
    "    \"\"\"\n",
    "    if backbone_name == \"efficientnet_b0\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        model = efficientnet_b0(weights=weights)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    elif backbone_name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "        model = resnet50(weights=weights)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    elif backbone_name == \"vit_b_16\":\n",
    "        weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "        model = vit_b_16(weights=weights)\n",
    "        in_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kx1O3maQL0U"
   },
   "source": [
    "# 6) Train / Eval Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1761924313643,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "WynRsgopQLpC"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += torch.sum(preds == labels).item()\n",
    "        total_count += imgs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc  = total_correct / total_count\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs  = []\n",
    "    all_preds  = []\n",
    "\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        total_loss   += loss.item() * imgs.size(0)\n",
    "        total_correct += torch.sum(preds == labels).item()\n",
    "        total_count  += imgs.size(0)\n",
    "\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_count\n",
    "    avg_acc  = total_correct / total_count\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs  = np.concatenate(all_probs)\n",
    "    all_preds  = np.concatenate(all_preds)\n",
    "\n",
    "    return avg_loss, avg_acc, all_labels, all_preds, all_probs\n",
    "\n",
    "\n",
    "def plot_training_curves(history, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    history dict:\n",
    "      \"train_loss\": [...],\n",
    "      \"val_loss\":   [...],\n",
    "      \"train_acc\":  [...],\n",
    "      \"val_acc\":    [...]\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Loss subplot\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, history[\"val_loss\"],   label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Acc subplot\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(epochs_range, history[\"val_acc\"],   label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred, y_prob, class_names):\n",
    "    \"\"\"\n",
    "    Returns dict of metrics, and also plots:\n",
    "      - Confusion matrix\n",
    "      - Per-class accuracy bar\n",
    "    Also computes ROC-AUC macro (OvR).\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    # ROC-AUC (macro, one-vs-rest)\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "    try:\n",
    "        roc_auc_macro = roc_auc_score(\n",
    "            y_true_bin, y_prob, average=\"macro\", multi_class=\"ovr\"\n",
    "        )\n",
    "    except ValueError:\n",
    "        roc_auc_macro = np.nan\n",
    "\n",
    "    # Confusion matrix plot\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cmap=\"Blues\"\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Per-class accuracy bar\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.barplot(x=class_names, y=per_class_acc)\n",
    "    plt.ylabel(\"Per-Class Accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(\"Per-Class Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_weighted\": precision,\n",
    "        \"recall_weighted\": recall,\n",
    "        \"f1_weighted\": f1,\n",
    "        \"roc_auc_macro\": roc_auc_macro,\n",
    "        \"per_class_accuracy\": dict(zip(class_names, per_class_acc)),\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwqCBDU7Qjq1"
   },
   "source": [
    "# 7) Run experiment for ONE split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5498,
     "status": "ok",
     "timestamp": 1761924325345,
     "user": {
      "displayName": "Freelancer SD",
      "userId": "05475058179902736823"
     },
     "user_tz": -360
    },
    "id": "XcLlYyvwQjXf"
   },
   "outputs": [],
   "source": [
    "!pip install -q thop\n",
    "from thop import profile\n",
    "\n",
    "def run_experiment_for_split(\n",
    "    train_ratio,\n",
    "    test_ratio,\n",
    "    all_samples,\n",
    "    backbone_name=\"efficientnet_b0\",\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    wd=1e-4,\n",
    "    verbose=True,\n",
    "    freeze_backbone=False,\n",
    "):\n",
    "    assert math.isclose(train_ratio + test_ratio, 1.0, rel_tol=1e-6), \"Train+Test must = 1.0\"\n",
    "\n",
    "    # ----- split data -----\n",
    "    idxs = list(range(len(all_samples)))\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        idxs,\n",
    "        train_size=train_ratio,\n",
    "        random_state=SEED,\n",
    "        shuffle=True,\n",
    "        stratify=[s[1] for s in all_samples]\n",
    "    )\n",
    "    train_samples = [all_samples[i] for i in train_idx]\n",
    "    test_samples  = [all_samples[i] for i in test_idx]\n",
    "\n",
    "    # datasets / loaders\n",
    "    ds_train = BetelLeafDataset(train_samples, transform=train_transform)\n",
    "    ds_test  = BetelLeafDataset(test_samples,  transform=test_transform)\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "    val_loader   = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # ----- model / loss / optim -----\n",
    "    model = get_model(backbone_name, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    # ðŸ”¥ FREEZE logic\n",
    "    if freeze_backbone:\n",
    "        # EfficientNet\n",
    "        if backbone_name == \"efficientnet_b0\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if not name.startswith(\"classifier\"):  # freeze everything except classifier\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # ResNet50\n",
    "        elif backbone_name == \"resnet50\":\n",
    "            for name, param in model.named_parameters():\n",
    "                if not name.startswith(\"fc\"):  # freeze all conv layers, train only fc\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # ViT\n",
    "        elif backbone_name == \"vit_b_16\":\n",
    "            for name, param in model.named_parameters():\n",
    "                # vit_b_16 has classifier at model.heads\n",
    "                if not name.startswith(\"heads\"):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        else:\n",
    "            # fallback: freeze all, then unfreeze last layer manually\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # compute GFLOPs & params once\n",
    "    dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "    gflops = macs / 1e9\n",
    "    total_params = params / 1e6  # in millions\n",
    "\n",
    "    # optimizer should only see trainable params\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=wd)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc   = 0.0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ----- training loop -----\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc, _, _, _ = eval_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{int(train_ratio*100)}-{int(test_ratio*100)}] \"\n",
    "                  f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "                  f\"TrainLoss={train_loss:.4f} ValLoss={val_loss:.4f} \"\n",
    "                  f\"TrainAcc={train_acc:.4f} ValAcc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # evaluate best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    val_loss, val_acc, y_true, y_pred, y_prob = eval_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "    # plots & metrics\n",
    "    plot_training_curves(history, title_prefix=f\"Train:{int(train_ratio*100)}% Test:{int(test_ratio*100)}%\")\n",
    "    class_names = [\"Healthy\", \"Diseased\", \"Dried\"]\n",
    "    metrics_dict = evaluate_metrics(y_true, y_pred, y_prob, class_names)\n",
    "\n",
    "    # add meta info\n",
    "    metrics_dict.update({\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "        \"final_val_loss\": float(val_loss),\n",
    "        \"final_val_acc\": float(val_acc),\n",
    "        \"GFLOPs\": float(gflops),\n",
    "        \"Params_M\": float(total_params),\n",
    "        \"Train_Time_sec\": float(total_time),\n",
    "        \"frozen_backbone\": bool(freeze_backbone),\n",
    "    })\n",
    "\n",
    "    # save checkpoint\n",
    "    ckpt_name = f\"checkpoint_{backbone_name}_{int(train_ratio*100)}_{int(test_ratio*100)}.pt\"\n",
    "    ckpt_path = os.path.join(OUTPUT_DIR, ckpt_name)\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epoch\": num_epochs,\n",
    "        \"history\": history,\n",
    "        \"class_to_idx\": class_to_idx,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "        \"random_seed\": SEED,\n",
    "        \"GFLOPs\": gflops,\n",
    "        \"Params_M\": total_params,\n",
    "        \"Train_Time_sec\": total_time,\n",
    "        \"frozen_backbone\": bool(freeze_backbone),\n",
    "    }, ckpt_path)\n",
    "\n",
    "    print(f\"Checkpoint saved to {ckpt_path}\")\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqZVbrYPSio3"
   },
   "source": [
    "# CELL 8: Execute one run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq1fTX3eSffO",
    "outputId": "bc812bdd-0520-4171-eb6e-d6a16365b1d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Running split Train:90% / Test:10% with efficientnet_b0\n",
      "============================================================\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 141MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90-10] Epoch 1/50 TrainLoss=0.7335 ValLoss=0.4348 TrainAcc=0.7042 ValAcc=0.8852\n",
      "[90-10] Epoch 2/50 TrainLoss=0.4944 ValLoss=0.3509 TrainAcc=0.8270 ValAcc=0.8947\n",
      "[90-10] Epoch 3/50 TrainLoss=0.4103 ValLoss=0.3042 TrainAcc=0.8596 ValAcc=0.9282\n",
      "[90-10] Epoch 4/50 TrainLoss=0.3745 ValLoss=0.3006 TrainAcc=0.8628 ValAcc=0.8804\n",
      "[90-10] Epoch 5/50 TrainLoss=0.3677 ValLoss=0.2635 TrainAcc=0.8612 ValAcc=0.9330\n",
      "[90-10] Epoch 6/50 TrainLoss=0.3196 ValLoss=0.2512 TrainAcc=0.8783 ValAcc=0.9234\n",
      "[90-10] Epoch 7/50 TrainLoss=0.3487 ValLoss=0.2393 TrainAcc=0.8617 ValAcc=0.9234\n",
      "[90-10] Epoch 8/50 TrainLoss=0.2969 ValLoss=0.2328 TrainAcc=0.8932 ValAcc=0.9378\n",
      "[90-10] Epoch 9/50 TrainLoss=0.2851 ValLoss=0.2438 TrainAcc=0.8948 ValAcc=0.9234\n",
      "[90-10] Epoch 10/50 TrainLoss=0.3041 ValLoss=0.2074 TrainAcc=0.8815 ValAcc=0.9330\n",
      "[90-10] Epoch 11/50 TrainLoss=0.3123 ValLoss=0.2315 TrainAcc=0.8788 ValAcc=0.9091\n",
      "[90-10] Epoch 12/50 TrainLoss=0.2753 ValLoss=0.1995 TrainAcc=0.8922 ValAcc=0.9330\n",
      "[90-10] Epoch 13/50 TrainLoss=0.2820 ValLoss=0.2154 TrainAcc=0.8948 ValAcc=0.9187\n",
      "[90-10] Epoch 14/50 TrainLoss=0.2850 ValLoss=0.2169 TrainAcc=0.8884 ValAcc=0.9234\n",
      "[90-10] Epoch 15/50 TrainLoss=0.2798 ValLoss=0.2031 TrainAcc=0.8884 ValAcc=0.9234\n",
      "[90-10] Epoch 16/50 TrainLoss=0.2826 ValLoss=0.1839 TrainAcc=0.8916 ValAcc=0.9282\n",
      "[90-10] Epoch 17/50 TrainLoss=0.2537 ValLoss=0.1903 TrainAcc=0.9034 ValAcc=0.9378\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(f\"Running split Train:{int(TRAIN_RATIO*100)}% / Test:{int(TEST_RATIO*100)}% with {BACKBONE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "single_result = run_experiment_for_split(\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    all_samples=all_samples,\n",
    "    backbone_name=BACKBONE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    wd=WEIGHT_DECAY,\n",
    "    verbose=True,\n",
    "    freeze_backbone=FREEZE_BACKBONE,\n",
    ")\n",
    "\n",
    "single_df = pd.DataFrame([{\n",
    "    \"backbone\": BACKBONE,\n",
    "    \"train%\": int(single_result[\"train_ratio\"]*100),\n",
    "    \"test%\": int(single_result[\"test_ratio\"]*100),\n",
    "    \"val_acc\": single_result[\"final_val_acc\"],\n",
    "    \"val_loss\": single_result[\"final_val_loss\"],\n",
    "    \"precision_weighted\": single_result[\"precision_weighted\"],\n",
    "    \"recall_weighted\": single_result[\"recall_weighted\"],\n",
    "    \"f1_weighted\": single_result[\"f1_weighted\"],\n",
    "    \"roc_auc_macro\": single_result[\"roc_auc_macro\"],\n",
    "    \"per_class_acc_Healthy\":  single_result[\"per_class_accuracy\"][\"Healthy\"],\n",
    "    \"per_class_acc_Diseased\": single_result[\"per_class_accuracy\"][\"Diseased\"],\n",
    "    \"per_class_acc_Dried\":    single_result[\"per_class_accuracy\"][\"Dried\"],\n",
    "    \"GFLOPs\": single_result[\"GFLOPs\"],\n",
    "    \"Params_M\": single_result[\"Params_M\"],\n",
    "    \"Train_Time_sec\": single_result[\"Train_Time_sec\"],\n",
    "    \"frozen_backbone\": single_result[\"frozen_backbone\"],\n",
    "}])\n",
    "\n",
    "display(single_df)\n",
    "\n",
    "csv_name = f\"task2_results_{BACKBONE}_{int(TRAIN_RATIO*100)}_{int(TEST_RATIO*100)}.csv\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "single_df.to_csv(csv_path, index=False)\n",
    "print(\"Saved summary CSV to:\", csv_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOPoh5e4A8o7sV0MRDfXmDW",
   "gpuType": "T4",
   "mount_file_id": "1ttkJpR1hWXWKhVQZQrtPLclSGD3cSvC1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
